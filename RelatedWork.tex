\vspace{-12pt}
\section{Related Work}
\vspace{-12pt}
\textbf{Error Correction Approaches:} EC tools can be mainly divided into three categories: $k$-spectrum based, suffix tree/array-based, and multiple sequence alignment-based (MSA) methods. Authors in \cite{pevzner2001eulerian} explained the procedure of using $k$-mers for error correction. %This involved first converting reads with insolid $k$-mers %, \ie, $k$-mers that occurred less than $M$ times in a given sequence dataset, 
%to solid ones with edit distance operations. As a result, the sequence then contains only solid $k$-mers. By identifying such $k$-mer sets, alignment is directly performed. 
While this work and others show that choosing a proper $k$ value results in a satisfactory overall assembly, no data-driven method is provided. Second, Suffix tree/array-based EC tools, \eg \cite{ilie2010hitec}, generalizes the $k$-mer based approach by handling multiple $k$ values and the corresponding threshold $M$. Finally, MSA research presented in \cite{salmela2011correcting} used $k$-mers as seeds and generated an adapted sequence that will be aligned further. Moreover, authors in \cite{chikhi2013informed} proposed an approach for finding the optimal $k$-values for genome assembly. However, the approach doesn't consider the optimal $k$-values for the error correction problem. Moreover, this approach depends on fitting histograms of unique k-mers and for some datasets it is not be able to fit the histogram to the generative model. ECHO \cite{kao2011echo} is an EC tool that doesn't require a user-defined k-mer value, it relies on perfoming multiple correction iterations with different values of $k$. However, only substitutions errors are considered and the runtime of the tool is reported to be very slow for some datasets (in terms of days) \cite{yang2012survey}. % and will not be able to find the best value of $k$. %Moreover, the proposed approach (as it depends on the abundance histograms of unique $k$-mers) cannot be easily adapted for other performance-critical parameters.
%Many open source EC tools are available, \eg Reptile~\cite{yang2010reptile}, Quake~\cite{kelley2010quake}, and PReptile~\cite{shah2012parallel}. All of these software modules measure three main metrics: 1) the accuracy of error correction, 2) runtime, and 3) memory usage. All three metrics can be affected by the selected value of $k$ and hence can be tuned with \name. 
%clarify above.. proposed approach
% Also authors in \cite {yang2012survey}  used ``gain'' metric to judge each system's performance, where gain is defined as the percentage of errors removed from the data set by the error-correction program, and thus requires a reference genome to estimate. However, it is not clear how the evaluation of the quality of correction is estimated in the absence of a reference genome.
\\
\textbf{Language Modeling in Genomics:} In the genomics domain, language modeling was used in \cite{ganapathiraju2002comparative} to find the characteristics of organisms in which N-gram analysis was applied to 44 different bacterial and archaeal genomes and to the human genome. 
% Moreover, \cite{ganapathiraju2012suite} 
In subsequent work, they used N-Gram language modeling for extracting patterns from whole genome sequences. Others~\cite{coin2003enhanced} have used language modeling to enhance domain recognition in protein sequences. For example, \cite{king2007ngloc} has used N-gram analysis specifically to create a Bayesian classifier to predict the localization of a protein sequence over 10 distinct eukaryotic organisms. 
RNNs can be thought of as a generalization of Hidden Markov Models (HMMs) and HMMs have been applied in several studies that seek to annotate epigenetic data. For example, \cite{song2015spectacle} presents a fast method of using spectral learning with HMMs for annotating chromatin states in the human genome.
Thus, we are seeing the use of various ML techniques traditionally used in NLP being used to make sense of biological data.  
\vspace{-5pt}
